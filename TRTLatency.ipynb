{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4fJKImP-V_K",
        "outputId": "82dea38e-2c0a-49d5-c3a9-30d19de051d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Fri Dec 27 05:01:48 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,630 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,517 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,840 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,448 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,566 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,633 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
            "Fetched 26.9 MB in 4s (6,697 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libnvinfer-headers-dev libnvinfer10\n",
            "The following NEW packages will be installed:\n",
            "  libnvinfer-dev libnvinfer-headers-dev libnvinfer-plugin8 libnvinfer10\n",
            "  libnvinfer8\n",
            "0 upgraded, 5 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 2,989 MB of archives.\n",
            "After this operation, 7,707 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-headers-dev 10.7.0.23-1+cuda12.6 [106 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer10 10.7.0.23-1+cuda12.6 [1,240 MB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dev 10.7.0.23-1+cuda12.6 [1,246 MB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer8 8.6.1.6-1+cuda12.0 [492 MB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-plugin8 8.6.1.6-1+cuda12.0 [11.7 MB]\n",
            "Fetched 2,989 MB in 37s (79.9 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libnvinfer-headers-dev.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../libnvinfer-headers-dev_10.7.0.23-1+cuda12.6_amd64.deb ...\n",
            "Unpacking libnvinfer-headers-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Selecting previously unselected package libnvinfer10.\n",
            "Preparing to unpack .../libnvinfer10_10.7.0.23-1+cuda12.6_amd64.deb ...\n",
            "Unpacking libnvinfer10 (10.7.0.23-1+cuda12.6) ...\n",
            "Selecting previously unselected package libnvinfer-dev.\n",
            "Preparing to unpack .../libnvinfer-dev_10.7.0.23-1+cuda12.6_amd64.deb ...\n",
            "Unpacking libnvinfer-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Selecting previously unselected package libnvinfer8.\n",
            "Preparing to unpack .../libnvinfer8_8.6.1.6-1+cuda12.0_amd64.deb ...\n",
            "Unpacking libnvinfer8 (8.6.1.6-1+cuda12.0) ...\n",
            "Selecting previously unselected package libnvinfer-plugin8.\n",
            "Preparing to unpack .../libnvinfer-plugin8_8.6.1.6-1+cuda12.0_amd64.deb ...\n",
            "Unpacking libnvinfer-plugin8 (8.6.1.6-1+cuda12.0) ...\n",
            "Setting up libnvinfer-headers-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Setting up libnvinfer10 (10.7.0.23-1+cuda12.6) ...\n",
            "Setting up libnvinfer-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Setting up libnvinfer8 (8.6.1.6-1+cuda12.0) ...\n",
            "Setting up libnvinfer-plugin8 (8.6.1.6-1+cuda12.0) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Cloning into 'tensorflow'...\n",
            "remote: Enumerating objects: 1940008, done.\u001b[K\n",
            "remote: Counting objects: 100% (1100/1100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (496/496), done.\u001b[K\n",
            "remote: Total 1940008 (delta 892), reused 610 (delta 604), pack-reused 1938908 (from 4)\u001b[K\n",
            "Receiving objects: 100% (1940008/1940008), 1.06 GiB | 17.01 MiB/s, done.\n",
            "Resolving deltas: 100% (1594270/1594270), done.\n",
            "Updating files: 100% (34443/34443), done.\n",
            "/content/tensorflow\n",
            "Updating files: 100% (27913/27913), done.\n",
            "Branch 'r2.10' set up to track remote branch 'r2.10' from 'origin'.\n",
            "Switched to a new branch 'r2.10'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bazel build --config=cuda --config=monolithic ... (Specify the build target with TensorRT support)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bazel install ... (Install the built TensorFlow package)'\n"
          ]
        }
      ],
      "source": [
        "# Ensure CUDA and cuDNN are installed\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "# Install the required dependencies for building TensorFlow with TensorRT support\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y libnvinfer8 libnvinfer-dev libnvinfer-plugin8\n",
        "# (Install other necessary packages as mentioned in TensorFlow documentation)\n",
        "# Clone the TensorFlow repository and checkout the desired branch\n",
        "!git clone https://github.com/tensorflow/tensorflow.git\n",
        "%cd tensorflow\n",
        "!git checkout r2.10 # Check the TensorFlow-TensorRT compatibility matrix for the correct branch.\n",
        "# Configure TensorFlow build with TensorRT enabled\n",
        "# ./configure\n",
        "# (During configuration, enable TensorRT support when prompted)\n",
        "# If you are using a virtual environment, activate it before building TensorFlow.\n",
        "# Build and install TensorFlow\n",
        "!bazel build --config=cuda --config=monolithic ... (Specify the build target with TensorRT support)\n",
        "!bazel install ... (Install the built TensorFlow package)\n",
        "# After successful installation, restart the runtime to ensure the new TensorFlow installation is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4Ue3OJZOygM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWukkuOg-gik",
        "outputId": "fd0ce05c-622e-4e02-d32f-e2c45dc6bd85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root\n"
          ]
        }
      ],
      "source": [
        "%cd ~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3QT2VaZLfYI",
        "outputId": "47111273-6f8a-44a5-90a8-02016b6eaa0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rVUneHDa-lEx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w4hBetqa-nDe"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Qtgz2-D1-ooq"
      },
      "outputs": [],
      "source": [
        "saved_model_dir = '/content/drive/MyDrive/model/model_trt'\n",
        "loaded_model = tf.saved_model.load(saved_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isMVUtL1-6QH",
        "outputId": "748ad789-9f2d-4189-aea9-b0f82300a557"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<ConcreteFunction (*, inputs: TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='inputs')) -> Dict[['output_0', TensorSpec(shape=(None, 10), dtype=tf.float32, name='output_0')]] at 0x7E061DDA14B0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = tf.experimental.tensorrt.ConversionParams(precision_mode='FP16')  # Use FP32 or INT8 as needed\n",
        "converter = tf.experimental.tensorrt.Converter(input_saved_model_dir='/content/drive/MyDrive/model/model_trt', conversion_params=params)\n",
        "converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BMJU4uNh-8kn"
      },
      "outputs": [],
      "source": [
        "optimized_model_dir = '/content/drive/MyDrive/test'\n",
        "converter.save(optimized_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mLBXaPCqFeWL"
      },
      "outputs": [],
      "source": [
        "model = loaded_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RlP-R_ni--WA"
      },
      "outputs": [],
      "source": [
        "# Original model\n",
        "original_model = loaded_model\n",
        "\n",
        "# TensorRT-optimized model\n",
        "optimized_model = tf.saved_model.load('/content/drive/MyDrive/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tdSbIGaq_AYz"
      },
      "outputs": [],
      "source": [
        "original_serving_fn = original_model.signatures['serving_default']\n",
        "optimized_serving_fn = optimized_model.signatures['serving_default']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5ruy_krr_B_c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample batch of data for benchmarking\n",
        "input_shape = (32, 32, 3)\n",
        "batch_size = 32\n",
        "dummy_input = np.random.random((batch_size, *input_shape)).astype(np.float32)\n",
        "original_input = tf.convert_to_tensor(dummy_input)\n",
        "optimized_input = tf.convert_to_tensor(dummy_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FTIdEywv_DWm"
      },
      "outputs": [],
      "source": [
        "def measure_latency(model_fn, input_key, input_tensor, warmup=10, iterations=100):\n",
        "\n",
        "    # Warm-up runs\n",
        "    for _ in range(warmup):\n",
        "        _ = model_fn(**{input_key: input_tensor})\n",
        "\n",
        "    # Measure latency\n",
        "    start_time = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = model_fn(**{input_key: input_tensor})\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate average latency\n",
        "    avg_latency = (end_time - start_time) / iterations\n",
        "    return avg_latency * 1000  # Convert to milliseconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gECzIgls_EiT"
      },
      "outputs": [],
      "source": [
        "original_latency = measure_latency(\n",
        "    original_serving_fn, \"inputs\", original_input\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mJ85LHOm_FzS"
      },
      "outputs": [],
      "source": [
        "optimized_latency = measure_latency(\n",
        "    optimized_serving_fn, \"inputs\", optimized_input\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qddQVdTI_G9q",
        "outputId": "7b7b7ad5-8347-4739-aa53-1087154f4857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Model Latency: 0.49 ms\n",
            "Optimized Model Latency: 0.45 ms\n"
          ]
        }
      ],
      "source": [
        "print(f\"Original Model Latency: {original_latency:.2f} ms\")\n",
        "print(f\"Optimized Model Latency: {optimized_latency:.2f} ms\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
